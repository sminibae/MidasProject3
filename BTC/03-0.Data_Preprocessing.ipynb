{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close time</th>\n",
       "      <th>Quote asset volume</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Taker buy base asset volume</th>\n",
       "      <th>Taker buy quote asset volume</th>\n",
       "      <th>plus_6</th>\n",
       "      <th>minus_6</th>\n",
       "      <th>zero_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1598918400000</td>\n",
       "      <td>11649.51</td>\n",
       "      <td>11668.50</td>\n",
       "      <td>11649.50</td>\n",
       "      <td>11667.12</td>\n",
       "      <td>165.713325</td>\n",
       "      <td>1598918579999</td>\n",
       "      <td>1.932464e+06</td>\n",
       "      <td>2146</td>\n",
       "      <td>83.409576</td>\n",
       "      <td>9.725705e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1598918580000</td>\n",
       "      <td>11667.12</td>\n",
       "      <td>11667.72</td>\n",
       "      <td>11634.06</td>\n",
       "      <td>11645.19</td>\n",
       "      <td>248.783097</td>\n",
       "      <td>1598918759999</td>\n",
       "      <td>2.897763e+06</td>\n",
       "      <td>3230</td>\n",
       "      <td>92.608664</td>\n",
       "      <td>1.078562e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1598918760000</td>\n",
       "      <td>11645.51</td>\n",
       "      <td>11649.88</td>\n",
       "      <td>11629.00</td>\n",
       "      <td>11639.12</td>\n",
       "      <td>217.735241</td>\n",
       "      <td>1598918939999</td>\n",
       "      <td>2.534315e+06</td>\n",
       "      <td>5186</td>\n",
       "      <td>93.190672</td>\n",
       "      <td>1.084609e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1598918940000</td>\n",
       "      <td>11639.12</td>\n",
       "      <td>11642.21</td>\n",
       "      <td>11630.21</td>\n",
       "      <td>11639.01</td>\n",
       "      <td>169.768550</td>\n",
       "      <td>1598919119999</td>\n",
       "      <td>1.975641e+06</td>\n",
       "      <td>3282</td>\n",
       "      <td>56.131767</td>\n",
       "      <td>6.532171e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1598919120000</td>\n",
       "      <td>11639.00</td>\n",
       "      <td>11640.01</td>\n",
       "      <td>11627.22</td>\n",
       "      <td>11634.16</td>\n",
       "      <td>130.973394</td>\n",
       "      <td>1598919299999</td>\n",
       "      <td>1.523519e+06</td>\n",
       "      <td>2300</td>\n",
       "      <td>45.500733</td>\n",
       "      <td>5.292799e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631196</th>\n",
       "      <td>1712620080000</td>\n",
       "      <td>71636.00</td>\n",
       "      <td>71683.82</td>\n",
       "      <td>71635.99</td>\n",
       "      <td>71676.45</td>\n",
       "      <td>22.694870</td>\n",
       "      <td>1712620259999</td>\n",
       "      <td>1.626456e+06</td>\n",
       "      <td>2027</td>\n",
       "      <td>10.817960</td>\n",
       "      <td>7.752474e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631197</th>\n",
       "      <td>1712620260000</td>\n",
       "      <td>71676.45</td>\n",
       "      <td>71676.46</td>\n",
       "      <td>71633.41</td>\n",
       "      <td>71636.83</td>\n",
       "      <td>15.908180</td>\n",
       "      <td>1712620439999</td>\n",
       "      <td>1.139815e+06</td>\n",
       "      <td>1209</td>\n",
       "      <td>5.646140</td>\n",
       "      <td>4.045180e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631198</th>\n",
       "      <td>1712620440000</td>\n",
       "      <td>71636.84</td>\n",
       "      <td>71636.84</td>\n",
       "      <td>71617.17</td>\n",
       "      <td>71621.38</td>\n",
       "      <td>24.054500</td>\n",
       "      <td>1712620619999</td>\n",
       "      <td>1.722863e+06</td>\n",
       "      <td>1326</td>\n",
       "      <td>7.466530</td>\n",
       "      <td>5.347731e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631199</th>\n",
       "      <td>1712620620000</td>\n",
       "      <td>71621.38</td>\n",
       "      <td>71662.94</td>\n",
       "      <td>71620.00</td>\n",
       "      <td>71620.00</td>\n",
       "      <td>42.037670</td>\n",
       "      <td>1712620799999</td>\n",
       "      <td>3.011572e+06</td>\n",
       "      <td>1624</td>\n",
       "      <td>17.243720</td>\n",
       "      <td>1.235209e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631200</th>\n",
       "      <td>1712620800000</td>\n",
       "      <td>71620.00</td>\n",
       "      <td>71692.00</td>\n",
       "      <td>71603.45</td>\n",
       "      <td>71684.01</td>\n",
       "      <td>69.406580</td>\n",
       "      <td>1712620979999</td>\n",
       "      <td>4.973102e+06</td>\n",
       "      <td>2489</td>\n",
       "      <td>52.086150</td>\n",
       "      <td>3.732187e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>631201 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open time      Open      High       Low     Close      Volume  \\\n",
       "0       1598918400000  11649.51  11668.50  11649.50  11667.12  165.713325   \n",
       "1       1598918580000  11667.12  11667.72  11634.06  11645.19  248.783097   \n",
       "2       1598918760000  11645.51  11649.88  11629.00  11639.12  217.735241   \n",
       "3       1598918940000  11639.12  11642.21  11630.21  11639.01  169.768550   \n",
       "4       1598919120000  11639.00  11640.01  11627.22  11634.16  130.973394   \n",
       "...               ...       ...       ...       ...       ...         ...   \n",
       "631196  1712620080000  71636.00  71683.82  71635.99  71676.45   22.694870   \n",
       "631197  1712620260000  71676.45  71676.46  71633.41  71636.83   15.908180   \n",
       "631198  1712620440000  71636.84  71636.84  71617.17  71621.38   24.054500   \n",
       "631199  1712620620000  71621.38  71662.94  71620.00  71620.00   42.037670   \n",
       "631200  1712620800000  71620.00  71692.00  71603.45  71684.01   69.406580   \n",
       "\n",
       "           Close time  Quote asset volume  Number of trades  \\\n",
       "0       1598918579999        1.932464e+06              2146   \n",
       "1       1598918759999        2.897763e+06              3230   \n",
       "2       1598918939999        2.534315e+06              5186   \n",
       "3       1598919119999        1.975641e+06              3282   \n",
       "4       1598919299999        1.523519e+06              2300   \n",
       "...               ...                 ...               ...   \n",
       "631196  1712620259999        1.626456e+06              2027   \n",
       "631197  1712620439999        1.139815e+06              1209   \n",
       "631198  1712620619999        1.722863e+06              1326   \n",
       "631199  1712620799999        3.011572e+06              1624   \n",
       "631200  1712620979999        4.973102e+06              2489   \n",
       "\n",
       "        Taker buy base asset volume  Taker buy quote asset volume  plus_6  \\\n",
       "0                         83.409576                  9.725705e+05       0   \n",
       "1                         92.608664                  1.078562e+06       0   \n",
       "2                         93.190672                  1.084609e+06       0   \n",
       "3                         56.131767                  6.532171e+05       0   \n",
       "4                         45.500733                  5.292799e+05       0   \n",
       "...                             ...                           ...     ...   \n",
       "631196                    10.817960                  7.752474e+05       0   \n",
       "631197                     5.646140                  4.045180e+05       0   \n",
       "631198                     7.466530                  5.347731e+05       0   \n",
       "631199                    17.243720                  1.235209e+06       0   \n",
       "631200                    52.086150                  3.732187e+06       0   \n",
       "\n",
       "        minus_6  zero_6  \n",
       "0             1       0  \n",
       "1             1       0  \n",
       "2             1       0  \n",
       "3             1       0  \n",
       "4             1       0  \n",
       "...         ...     ...  \n",
       "631196        0       1  \n",
       "631197        0       1  \n",
       "631198        0       1  \n",
       "631199        0       1  \n",
       "631200        0       1  \n",
       "\n",
       "[631201 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/df_all.csv')\n",
    "data = data.drop(['Ignore','up_cross','down_cross','minutes','log_minutes','side'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ISO 8601 date strings to Unix timestamp (milliseconds)\n",
    "def iso_to_unix(iso_str):\n",
    "    dt = datetime.strptime(iso_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "    return int(dt.timestamp() * 1000)  # Convert to milliseconds\n",
    "\n",
    "def unix_to_iso(unix_timestamp_ms):\n",
    "    # Convert milliseconds to seconds\n",
    "    unix_timestamp_s = unix_timestamp_ms / 1000\n",
    "    # Create a datetime object from the Unix timestamp\n",
    "    dt = datetime.utcfromtimestamp(unix_timestamp_s)\n",
    "    # Format the datetime object as an ISO 8601 date string\n",
    "    iso_str = dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return iso_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong! 43319\n",
      "1606715820000\n",
      "wrong! 53535\n",
      "1608558300000\n",
      "wrong! 55135\n",
      "1608861420000\n",
      "wrong! 78189\n",
      "1613014740000\n",
      "wrong! 89169\n",
      "1614995820000\n",
      "wrong! 110739\n",
      "1618883820000\n",
      "wrong! 113130\n",
      "1619323200000\n",
      "wrong! 165795\n",
      "1628819820000\n",
      "wrong! 188365\n",
      "1632898620000\n",
      "wrong! 448119\n",
      "1679661540000\n"
     ]
    }
   ],
   "source": [
    "# looking for not continuous points\n",
    "non_continuous_index =[]\n",
    "\n",
    "for i in range(len(data)-1):\n",
    "    if data['Open time'][i+1] - data['Open time'][i] != 3*60*1000:\n",
    "        print('wrong!', i)\n",
    "        print(data['Open time'][i])\n",
    "        non_continuous_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43319, 53535, 55135, 78189, 89169, 110739, 113130, 165795, 188365, 448119]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(non_continuous_index)\n",
    "print(len(non_continuous_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold your chunks of data\n",
    "data_chunks = []\n",
    "\n",
    "# Set the start index for the first chunk\n",
    "start_idx = 0\n",
    "\n",
    "# Iterate through the non-continuous indices and split the data into chunks\n",
    "for idx in non_continuous_index:\n",
    "    # Create a chunk from start_idx to the non-continuous index\n",
    "    chunk = data.iloc[start_idx:idx+1]\n",
    "    # Append the chunk to your list of data chunks\n",
    "    data_chunks.append(chunk.reset_index(drop=True))\n",
    "    # Update start_idx for the next chunk\n",
    "    start_idx = idx + 1\n",
    "\n",
    "# Don't forget to grab the last chunk of data after the last non-continuous index\n",
    "final_chunk = data.iloc[start_idx:]\n",
    "data_chunks.append(final_chunk.reset_index(drop=True))\n",
    "\n",
    "# Now data_chunks is a list of DataFrames, each representing a continuous chunk of time\n",
    "# You can access individual chunks with data_chunks[0], data_chunks[1], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window sizes for the moving averages\n",
    "windows = [5, 10, 20, 30, 60, 120, 240]\n",
    "# Define the window size and standard deviation multiplier for the Bollinger Bands\n",
    "BB_window_size = 90\n",
    "BB_std_multiplier = 1\n",
    "\n",
    "# Iterate through each chunk in data_chunks\n",
    "for i, chunk in enumerate(data_chunks):\n",
    "    # ADDING MA\n",
    "    for window in windows:\n",
    "        # Calculate the moving average\n",
    "        moving_avg = chunk['Close'].rolling(window).mean()\n",
    "        # Add the moving average as a new column to the chunk\n",
    "        chunk[f'MA{window}'] = moving_avg\n",
    "\n",
    "    # Calculate the moving average and standard deviation\n",
    "    BB_moving_avg = chunk['Close'].rolling(BB_window_size).mean()\n",
    "    BB_std_dev = chunk['Close'].rolling(BB_window_size).std()\n",
    "\n",
    "    # ADDING BB\n",
    "    # Calculate the Bollinger Bands\n",
    "    BB_upper_band = BB_moving_avg + (BB_std_multiplier * BB_std_dev)\n",
    "    BB_lower_band = BB_moving_avg - (BB_std_multiplier * BB_std_dev)\n",
    "\n",
    "    # Add the Bollinger Bands and moving average as new columns to the chunk\n",
    "    chunk[f'MA{BB_window_size}'] = BB_moving_avg\n",
    "    chunk[f'Upper_Band{BB_window_size}'] = BB_upper_band\n",
    "    chunk[f'Lower_Band{BB_window_size}'] = BB_lower_band\n",
    "\n",
    "    # drop NaN values\n",
    "    chunk = chunk.dropna().reset_index(drop=True)\n",
    "    chunk = chunk.drop(columns = ['Open time', 'Close time',], axis=1)\n",
    "    # Optionally, update the chunk in data_chunks (if you want to keep the changes)\n",
    "    data_chunks[i] = chunk\n",
    "\n",
    "# Now each chunk in data_chunks has new columns for the moving averages\n",
    "\n",
    "\n",
    "# # If you prefer to have separate variables for each chunk, you could do something like:\n",
    "# for i, chunk in enumerate(data_chunks):\n",
    "#     globals()[f'data_chunk_{i}'] = chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize empty lists to hold the data\n",
    "matrix_list = []\n",
    "answer_list = []\n",
    "\n",
    "for chunk in data_chunks:\n",
    "    if len(chunk)>=20:\n",
    "        for i in range(len(chunk) - 19):  # Ensure there are 20 rows available\n",
    "            # Create a matrix of 20 rows\n",
    "            matrix = chunk.drop(columns= ['plus_6','minus_6','zero_6'], axis=1).iloc[i:i+20].values\n",
    "            matrix_list.append(matrix)\n",
    "\n",
    "            # Get the up, down, zero values and convert them to a list\n",
    "            answer = chunk.iloc[i+19][['plus_6', 'minus_6', 'zero_6']].tolist()\n",
    "            answer_list.append(answer)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "matrix_array_20 = np.array(matrix_list)\n",
    "answer_array_20 = np.array(answer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to 03-1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back to maxtrix_array_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_array_20 = np.load('Data/matrix_array_20.npy')\n",
    "# answer_array_20 = np.load('Data/answer_array_20.npy')\n",
    "\n",
    "# List of file names to load\n",
    "file_names_matrix = [f\"Data/matrix_array_20_{i}.npy\" for i in range(11)]  # Adjust range as needed\n",
    "# file_names_answer = [f\"Data/answer_array_20_{i}.npy\" for i in range(11)]\n",
    "\n",
    "# Load each file and store in a list\n",
    "loaded_arrays_matrix = [np.load(file_name) for file_name in file_names_matrix]\n",
    "# loaded_arrays_answer = [np.load(file_name) for file_name in file_names_answer]\n",
    "\n",
    "# Concatenate all arrays into a single array\n",
    "matrix_array_20 = np.concatenate(loaded_arrays_matrix, axis=0)\n",
    "# answer_array_20 = np.concatenate(loaded_arrays_answer, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587563, 20, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix_array_20.shape, answer_array_20.shape\n",
    "matrix_array_20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/matrix_array_20.npy', matrix_array_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scalers/StandardScaler_20.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming data is your 600k matrices concatenated into a single 3D numpy array of shape (600000, 20, 19)\n",
    "matrix_array_20_reshaped = matrix_array_20.reshape(-1, 19)  # Reshape to 2D for standardization\n",
    "scaler = StandardScaler()\n",
    "matrix_array_20_normalized = scaler.fit_transform(matrix_array_20_reshaped)\n",
    "\n",
    "# Reshape back to 3D\n",
    "matrix_array_20_normalized = matrix_array_20_normalized.reshape(-1, 20, 19)\n",
    "\n",
    "# save that scaler\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, 'Scalers/StandardScaler_20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data/matrix_array_20_normalized.npy', matrix_array_20_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back to maxtrix_array_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array_40 = np.load('Data/matrix_array_40.npy')\n",
    "answer_array_40 = np.load('Data/answer_array_40.npy')\n",
    "\n",
    "# List of file names to load\n",
    "# file_names_matrix = [f\"Data/matrix_array_40_{i}.npy\" for i in range(11)]  # Adjust range as needed\n",
    "# file_names_answer = [f\"Data/answer_array_40_{i}.npy\" for i in range(11)]\n",
    "\n",
    "# # Load each file and store in a list\n",
    "# loaded_arrays_matrix = [np.load(file_name) for file_name in file_names_matrix]\n",
    "# loaded_arrays_answer = [np.load(file_name) for file_name in file_names_answer]\n",
    "\n",
    "# # Concatenate all arrays into a single array\n",
    "# matrix_array_40 = np.concatenate(loaded_arrays_matrix, axis=0)\n",
    "# answer_array_40 = np.concatenate(loaded_arrays_answer, axis=0)\n",
    "\n",
    "# np.save('Data/matrix_array_40.npy', matrix_array_40)\n",
    "# np.save('Data/answer_array_40.npy', answer_array_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((587343, 40, 19), (587343, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_array_40.shape, answer_array_40.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming data is your 600k matrices concatenated into a single 3D numpy array of shape (600000, 20, 19)\n",
    "matrix_array_40_reshaped = matrix_array_40.reshape(-1, 19)  # Reshape to 2D for standardization\n",
    "scaler = StandardScaler()\n",
    "matrix_array_40_normalized = scaler.fit_transform(matrix_array_40_reshaped)\n",
    "\n",
    "# Reshape back to 3D\n",
    "matrix_array_40_normalized = matrix_array_40_normalized.reshape(-1, 40, 19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save that scaler\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, 'Scalers/StandardScaler_40.pkl')\n",
    "\n",
    "np.save('Data/matrix_array_40_normalized.npy', matrix_array_40_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back to maxtrix_array_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array_60 = np.load('Data/matrix_array_60.npy')\n",
    "answer_array_60 = np.load('Data/answer_array_60.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((587123, 60, 19), (587123, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_array_60.shape, answer_array_60.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming data is your 600k matrices concatenated into a single 3D numpy array of shape (600000, 20, 19)\n",
    "matrix_array_60_reshaped = matrix_array_60.reshape(-1, 19)  # Reshape to 2D for standardization\n",
    "scaler = StandardScaler()\n",
    "matrix_array_60_normalized = scaler.fit_transform(matrix_array_60_reshaped)\n",
    "\n",
    "# Reshape back to 3D\n",
    "matrix_array_60_normalized = matrix_array_60_normalized.reshape(-1, 60, 19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save that scaler\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, 'Scalers/StandardScaler_60.pkl')\n",
    "\n",
    "np.save('Data/matrix_array_60_normalized.npy', matrix_array_60_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back to maxtrix_array_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array_80 = np.load('Data/matrix_array_80.npy')\n",
    "answer_array_80 = np.load('Data/answer_array_80.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((586903, 80, 19), (586903, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_array_80.shape, answer_array_80.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming data is your 600k matrices concatenated into a single 3D numpy array of shape (600000, 20, 19)\n",
    "matrix_array_80_reshaped = matrix_array_80.reshape(-1, 19)  # Reshape to 2D for standardization\n",
    "scaler = StandardScaler()\n",
    "matrix_array_80_normalized = scaler.fit_transform(matrix_array_80_reshaped)\n",
    "\n",
    "# Reshape back to 3D\n",
    "matrix_array_80_normalized = matrix_array_80_normalized.reshape(-1, 80, 19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save that scaler\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, 'Scalers/StandardScaler_80.pkl')\n",
    "\n",
    "np.save('Data/matrix_array_80_normalized.npy', matrix_array_80_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
