{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary\n",
    "from tqdm import  tqdm\n",
    "\n",
    "import os, json, gc, io, joblib\n",
    "from contextlib import redirect_stdout\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Multi-layer LSTM\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=3, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # Flatten layer \n",
    "\n",
    "        # Define Linear layers with Batch Normalization, GELU, and Dropout\n",
    "        self.linear1 = nn.Linear(hidden_dim, 64)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(32)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(16)\n",
    "        self.gelu3 = nn.GELU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through LSTM layers\n",
    "        # print(\"Output type1:\", type(x))\n",
    "        (lstm_out, _) = self.lstm(x)\n",
    "\n",
    "        # Taking the output of the last time step\n",
    "        # print(\"Output type2:\", type(x))\n",
    "        x = lstm_out[:, -1, :]\n",
    "\n",
    "        # Pass through Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.gelu3(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUs (Gated Recurrent Units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Flatten layer \n",
    "\n",
    "        # Define Linear layers with Batch Normalization, GELU, and Dropout\n",
    "        self.linear1 = nn.Linear(hidden_dim, 64)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(32)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(16)\n",
    "        self.gelu3 = nn.GELU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through GRU layers\n",
    "        # print(\"Output type1:\", type(x))\n",
    "        gru_out, _ = self.gru(x)\n",
    "\n",
    "        # Taking the output of the last time step\n",
    "        # print(\"Output type2:\", type(x))\n",
    "        x = gru_out[:, -1, :]\n",
    "\n",
    "        # Pass through Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.gelu3(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Conv1DModel, self).__init__()\n",
    "\n",
    "        # Conv1D Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(32)  \n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Dropout\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)  \n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout2 = nn.Dropout(0.2)  # Dropout\n",
    "        \n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)  \n",
    "        self.gelu3 = nn.GELU()\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout3 = nn.Dropout(0.2)  # Dropout\n",
    "        \n",
    "\n",
    "        # Flatten layer \n",
    "        self.seq_length_after_conv_and_pool = 20 // 2 // 2 // 2 # Pooling 3 times with stride 2\n",
    "\n",
    "        # Define Linear layers with Batch Normalization, GELU, and Dropout\n",
    "        self.linear1 = nn.Linear(128 * self.seq_length_after_conv_and_pool, 64)\n",
    "        self.batch_norm_lin1 = nn.BatchNorm1d(64)\n",
    "        self.gelu_lin1 = nn.GELU()\n",
    "        self.dropout_lin1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.batch_norm_lin2 = nn.BatchNorm1d(32)\n",
    "        self.gelu_lin2 = nn.GELU()\n",
    "        self.dropout_lin2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.batch_norm_lin3 = nn.BatchNorm1d(16)\n",
    "        self.gelu_lin3 = nn.GELU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(16, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x shape is (batch_size, seq_length, num_features)\n",
    "        # Conv1d expects (batch_size, in_channels, seq_length), so transpose x\n",
    "        x = x.transpose(1, 2)  # Now x shape: (batch_size, num_features, seq_length)\n",
    "\n",
    "        # Apply Conv1D layers followed by pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.gelu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * self.seq_length_after_conv_and_pool)\n",
    "\n",
    "        # Pass through Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm_lin1(x)\n",
    "        x = self.gelu_lin1(x)\n",
    "        x = self.dropout_lin1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm_lin2(x)\n",
    "        x = self.gelu_lin2(x)\n",
    "        x = self.dropout_lin2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batch_norm_lin3(x)\n",
    "        x = self.gelu_lin3(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_length, num_classes, d_model=64, nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Input embedding layer\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Positional Encoding (Not using nn.Embedding here to keep it simple)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, seq_length)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "\n",
    "        # Define Linear layers with Batch Normalization, GELU, and Dropout\n",
    "        self.linear1 = nn.Linear(seq_length * d_model, 64)\n",
    "        self.batch_norm_lin1 = nn.BatchNorm1d(64)\n",
    "        self.gelu_lin1 = nn.GELU()\n",
    "        self.dropout_lin1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.batch_norm_lin2 = nn.BatchNorm1d(32)\n",
    "        self.gelu_lin2 = nn.GELU()\n",
    "        self.dropout_lin2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.batch_norm_lin3 = nn.BatchNorm1d(16)\n",
    "        self.gelu_lin3 = nn.GELU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(16, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Assuming src shape is (batch_size, seq_length, input_dim)\n",
    "        # Transformer expects (seq_length, batch_size, input_dim), so transpose src\n",
    "        src = src.transpose(0, 1)\n",
    "\n",
    "        # Embedding and positional encoding\n",
    "        src = self.embedding(src)  # Now shape is (seq_length, batch_size, d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        # Transformer\n",
    "        output = self.transformer(src)\n",
    "\n",
    "        # For linear layers, we'll consider the output of all positions.\n",
    "        # Reshape output to (batch_size, seq_length * d_model) before passing to linear layers.\n",
    "        # Note: Adjusting this as per the expected input for linear layers.\n",
    "        output = output.transpose(0, 1)  # Change back to (batch_size, seq_length, d_model)\n",
    "        x = output.flatten(start_dim=1)\n",
    "\n",
    "        # Pass through Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm_lin1(x)\n",
    "        x = self.gelu_lin1(x)\n",
    "        x = self.dropout_lin1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm_lin2(x)\n",
    "        x = self.gelu_lin2(x)\n",
    "        x = self.dropout_lin2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batch_norm_lin3(x)\n",
    "        x = self.gelu_lin3(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, seq_length, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Calculate the flattened input size\n",
    "        self.flattened_size = seq_length * input_dim\n",
    "        \n",
    "        # Define Linear layers with Batch Normalization, GELU, and Dropout\n",
    "        self.linear1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.batch_norm_lin1 = nn.BatchNorm1d(128)\n",
    "        self.gelu_lin1 = nn.GELU()\n",
    "        self.dropout_lin1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear2 = nn.Linear(128, 48)\n",
    "        self.batch_norm_lin2 = nn.BatchNorm1d(48)\n",
    "        self.gelu_lin2 = nn.GELU()\n",
    "        self.dropout_lin2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear3 = nn.Linear(48, 16)\n",
    "        self.batch_norm_lin3 = nn.BatchNorm1d(16)\n",
    "        self.gelu_lin3 = nn.GELU()\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(16, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, self.flattened_size)  # Reshape input to (batch_size, seq_length*input_dim)\n",
    "        \n",
    "        # Pass through Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm_lin1(x)\n",
    "        x = self.gelu_lin1(x)\n",
    "        x = self.dropout_lin1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm_lin2(x)\n",
    "        x = self.gelu_lin2(x)\n",
    "        x = self.dropout_lin2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batch_norm_lin3(x)\n",
    "        x = self.gelu_lin3(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(seq_length):\n",
    "    matrix_array = np.load(f'Data/matrix_array_{seq_length}_normalized.npy')\n",
    "    answer_array = np.load(f'Data/answer_array_{seq_length}}.npy')\n",
    "\n",
    "    labels = torch.tensor(answer_array)\n",
    "    indices = torch.argmax(labels, dim=1)\n",
    "    mapped_labels = torch.tensor([1 if i == 0 else 2 if i == 1 else 0 for i in indices])\n",
    "    mapped_labels\n",
    "\n",
    "    # answer = chunk.iloc[i+19][['plus_6', 'minus_6', 'zero_6']].tolist()\n",
    "    # 1 = up , 2 = down, 0 = zero\n",
    "\n",
    "    X = matrix_array\n",
    "    y = mapped_labels\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X,y, test_size=0.2, random_state=1, stratify=y)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use torch.long for labels if using CrossEntropyLoss\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    batch_size = 256  # You can adjust the batch size as needed\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Now your DataLoaders are ready to be used in the training loop\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, model_instance, seq_length):\n",
    "\n",
    "    _, __, test_loader = import_data(seq_length)\n",
    "    \n",
    "    # Setting device to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = model_instance\n",
    "    state_dict = torch.load(f'Models/{model_name}_model_state_dict.pth')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Initialize necessary metrics\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # No need to track gradients for validation, which saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        # Wrap your loader with tqdm for a progress bar\n",
    "        pbar_test = tqdm(enumerate(test_loader), total=len(test_loader), desc=f\"Epoch 1/1\")\n",
    "        for i, (images, labels) in pbar_test:\n",
    "            # Move tensors to the same device as model\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())  # Move back to cpu and convert to numpy\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # Update progress bar\n",
    "            # pbar_test.set_postfix({'loss': running_loss / (i + 1)})\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "    \n",
    "    # Calculating Precision, Recall, F1 Score, and Confusion Matrix\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')  # or other averaging method\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Printing the metrics\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Plot the normalized confusion matrix\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    with open(f'Models/{model_name}_history.json', 'r') as f:\n",
    "        history = json.load(f)  \n",
    "\n",
    "    # Extracting values for plotting\n",
    "    train_loss = history['train_loss']\n",
    "    val_loss = history['val_loss']\n",
    "    train_accuracy = history['train_accuracy']\n",
    "    val_accuracy = history['val_accuracy']\n",
    "    epochs = range(1, len(train_loss) + 1)  # 1, 2, ... , num_epochs\n",
    "    last_improvement_epoch = len(train_loss) - 5\n",
    "\n",
    "    # Creating subplots for loss and accuracy\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    ax[0].plot(epochs, train_loss, 'r', label='Training loss')\n",
    "    ax[0].plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    ax[0].axvline(x=last_improvement_epoch, color='g', linestyle='--', label='Last Improvement') \n",
    "    ax[0].set_title('Training and Validation Loss')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    ax[1].plot(epochs, train_accuracy, 'r', label='Training accuracy')\n",
    "    ax[1].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "    ax[1].axvline(x=last_improvement_epoch, color='g', linestyle='--', label='Last Improvement')\n",
    "    ax[1].set_title('Training and Validation Accuracy')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [20,40,60,80]\n",
    "\n",
    "for seq_length in seq_lengths:\n",
    "    \n",
    "    models = {\n",
    "        'LSTM' : LSTMModel(input_dim=19, hidden_dim=128, output_dim=3),\n",
    "        'GRU' : GRUModel(input_dim=19, hidden_dim=128, output_dim=3, num_layers=3),\n",
    "        'Conv1D' : Conv1DModel(num_features=19, output_dim=3, seq_length=seq_length),\n",
    "        'Transformer' :  TransformerModel(input_dim=19, output_dim=3, seq_length=seq_length, num_classes=3, \\\n",
    "                                        d_model=64, nhead=4, num_encoder_layers=2, dim_feedforward=256, dropout=0.1),\n",
    "        \"Linear\" : LinearModel(seq_length=seq_length, input_dim=19, output_dim=3),\n",
    "    }   \n",
    "    \n",
    "    for model_name, model_instance in models.items():\n",
    "        test_model(model_name, model_instance, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
